{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the consumer complaint dataset\ndf = pd.read_csv('/kaggle/input/complaints/complaints.csv')\nprint(df.head())\n\ndf.columns = df.columns.str.replace(' ', '_')\ncolumn_names = df.columns\nprint(column_names)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore the distribution of product categories\nproduct_category_counts = df['Product'].value_counts()\nplt.figure(figsize=(8, 6))\nproduct_category_counts.plot(kind='bar')\nplt.title('Distribution of Product Categories')\nplt.xlabel('Product Category')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure the 'Consumer_complaint_narrative' column is recognized as string type\ndf['Consumer_complaint_narrative'] = df['Consumer_complaint_narrative'].astype(str)\n\n# Perform text pre-processing\ndf['Consumer_complaint_narrative'] = df['Consumer_complaint_narrative'].str.lower()\ndf['Consumer_complaint_narrative'] = df['Consumer_complaint_narrative'].str.replace('[^a-zA-Z\\s]', '')\n\n# Tokenization (split text into words)\ndf['Consumer_complaint_tokens'] = df['Consumer_complaint_narrative'].str.split()\n\n# Explore the distribution of text lengths\ndf['Text_Length'] = df['Consumer_complaint_tokens'].apply(len)\nplt.figure(figsize=(8, 6))\nplt.hist(df['Text_Length'], bins=30, alpha=0.5)\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Consumer_complaint_narrative'] = df['Consumer_complaint_narrative'].astype(str)\n\n# Text Pre-processing\ndef preprocess_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    \n    # Remove punctuation and special characters\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove extra whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Apply text pre-processing to the 'Consumer_complaint_narrative' column\ndf['Consumer_complaint_narrative'] = df['Consumer_complaint_narrative'].apply(preprocess_text)\n\n# Tokenization (split text into words)\ndf['Consumer_complaint_tokens'] = df['Consumer_complaint_narrative'].str.split()\n\n# Explore the distribution of text lengths\ndf['Text_Length'] = df['Consumer_complaint_tokens'].apply(len)\nplt.figure(figsize=(8, 6))\nplt.hist(df['Text_Length'], bins=30, alpha=0.5)\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Split the dataset into training and testing sets\nX = df['Consumer_complaint_narrative']\ny = df['Product']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 1: Multinomial Naive Bayes\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train_tfidf, y_train)\nnb_predictions = nb_classifier.predict(X_test_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 2: Logistic Regression\nlogreg_classifier = LogisticRegression(max_iter=1000)\nlogreg_classifier.fit(X_train_tfidf, y_train)\nlogreg_predictions = logreg_classifier.predict(X_test_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 3: Support Vector Machines (SVM)\nsvm_classifier = SVC(kernel='linear', C=1, probability=True)\nsvm_classifier.fit(X_train_tfidf, y_train)\nsvm_predictions = svm_classifier.predict(X_test_tfidf)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the models\ndef evaluate_model(y_true, y_pred, model_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    report = classification_report(y_true, y_pred)\n    print(f\"Model: {model_name}\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(\"Classification Report:\\n\", report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the SVM model\nevaluate_model(y_test, svm_predictions, \"Support Vector Machines (SVM)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(y_test, nb_predictions, \"Multinomial Naive Bayes\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(y_test, logreg_predictions, \"Logistic Regression\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\n# Evaluate the models\ndef evaluate_model(y_true, y_pred, model_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    report = classification_report(y_true, y_pred)\n    return {\n        \"Model\": model_name,\n        \"Accuracy\": accuracy,\n        \"Classification Report\": report\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 1: Multinomial Naive Bayes\nnb_predictions = nb_classifier.predict(X_test_tfidf)\nnb_performance = evaluate_model(y_test, nb_predictions, \"Multinomial Naive Bayes\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 2: Logistic Regression\nlogreg_predictions = logreg_classifier.predict(X_test_tfidf)\nlogreg_performance = evaluate_model(y_test, logreg_predictions, \"Logistic Regression\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 3: Support Vector Machines (SVM)\nsvm_predictions = svm_classifier.predict(X_test_tfidf)\nsvm_performance = evaluate_model(y_test, svm_predictions, \"Support Vector Machines (SVM)\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare performance\nperformance_metrics = [nb_performance, logreg_performance, svm_performance]\n\nfor performance in performance_metrics:\n    print(f\"Model: {performance['Model']}\")\n    print(f\"Accuracy: {performance['Accuracy']:.2f}\")\n    print(\"Classification Report:\\n\", performance['Classification Report'])\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Function to evaluate and print model performance\ndef evaluate_model(y_true, y_pred, model_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    report = classification_report(y_true, y_pred)\n    matrix = confusion_matrix(y_true, y_pred)\n    \n    print(f\"Model: {model_name}\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(\"Classification Report:\\n\", report)\n    print(\"Confusion Matrix:\\n\", matrix)\n\n# Evaluate Multinomial Naive Bayes\nnb_predictions = nb_classifier.predict(X_test_tfidf)\nevaluate_model(y_test, nb_predictions, \"Multinomial Naive Bayes\")\n\n# Evaluate Logistic Regression\nlogreg_predictions = logreg_classifier.predict(X_test_tfidf)\nevaluate_model(y_test, logreg_predictions, \"Logistic Regression\")\n\n# Evaluate Support Vector Machines (SVM)\nsvm_predictions = svm_classifier.predict(X_test_tfidf)\nevaluate_model(y_test, svm_predictions, \"Support Vector Machines (SVM)\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example text to predict\ntext_to_predict = \"\"\n\n# Preprocess the text (same preprocessing steps as during training)\ntext_to_predict = preprocess_text(text_to_predict)\n\n# Vectorize the preprocessed text using the same TF-IDF vectorizer\ntext_to_predict_tfidf = tfidf_vectorizer.transform([text_to_predict])\n\n# Make predictions using the selected model\npredicted_category = model.predict(text_to_predict_tfidf)\n\n# Decode the predicted category into a human-readable label (e.g., 'Credit reporting, repair, or other')\npredicted_category_label = label_encoder.inverse_transform(predicted_category)\n\n# Print the predicted category\nprint(f\"Predicted Category: {predicted_category_label[0]}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}